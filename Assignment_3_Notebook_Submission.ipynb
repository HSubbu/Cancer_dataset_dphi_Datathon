{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkDHG6SaWBb"
   },
   "source": [
    "## Datathon-2: Notebook Submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content**\n",
    "\n",
    "As per WHO,\n",
    "\n",
    "Cancer is the second leading cause of death globally, and is responsible for an estimated 9.6 million deaths in 2018. Globally, about 1 in 6 deaths is due to cancer.\n",
    "Approximately 70% of deaths from cancer occur in low- and middle-income countries.\n",
    "Around one third of deaths from cancer are due to the 5 leading behavioral and dietary risks: high body mass index, low fruit and vegetable intake, lack of physical activity, tobacco use, and alcohol use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement**\n",
    "\n",
    "Many aspects of the behaviour of cancer disease are highly unpredictable. Even with the huge number of studies that have been done on the DNA mutation responsible for the disease, we are still unable to use these information at clinical level. However, it is important that we understand the effects and impacts of this disease from the past information as much as we possibly can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "You are required to build a machine learning  model that would predict the cancer death rate for the given year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Criteria**\n",
    "\n",
    "Submissions are evaluated using Mean Squared Error (MSE).\n",
    "\n",
    "https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/mse.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the data**\n",
    "\n",
    "The data is collected from cancer.gov and the US Census American Community Survey. There are 34 columns including the target column. Some of the columns are listed below:\n",
    "\n",
    "TARGET_deathRate: Dependent variable. Mean per capita (100,000) cancer mortalities(a)\n",
    "avgAnnCount: Mean number of reported cases of cancer diagnosed annually(a)\n",
    "avgDeathsPerYear: Mean number of reported mortalities due to cancer(a)\n",
    "incidenceRate: Mean per capita (100,000) cancer diagoses(a)\n",
    "medianIncome: Median income per county (b)\n",
    "popEst2015: Population of county (b)\n",
    "povertyPercent: Percent of populace in poverty (b)\n",
    "studyPerCap: Per capita number of cancer-related clinical trials per county (a)\n",
    "binnedInc: Median income per capita binned by decile (b)\n",
    "MedianAge: Median age of county residents (b)\n",
    "MedianAgeMale: Median age of male county residents (b)\n",
    "MedianAgeFemale: Median age of female county residents (b)\n",
    "Geography: County name (b)\n",
    "AvgHouseholdSize: Mean household size of county (b)\n",
    "PercentMarried: Percent of county residents who are married (b)\n",
    "PctNoHS18_24: Percent of county residents ages 18-24 highest education attained: less than high school (b)\n",
    "PctHS18_24: Percent of county residents ages 18-24 highest education attained: high school diploma (b)\n",
    "PctSomeCol18_24: Percent of county residents ages 18-24 highest education attained: some college (b)\n",
    "PctBachDeg18_24: Percent of county residents ages 18-24 highest education attained: bachelor's degree (b)\n",
    "PctHS25_Over: Percent of county residents ages 25 and over highest education attained: high school diploma (b)\n",
    "PctBachDeg25_Over: Percent of county residents ages 25 and over highest education attained: bachelor's degree (b)\n",
    "PctEmployed16_Over: Percent of county residents ages 16 and over employed (b)\n",
    "PctUnemployed16_Over: Percent of county residents ages 16 and over unemployed (b)\n",
    "PctPrivateCoverage: Percent of county residents with private health coverage (b)\n",
    "PctPrivateCoverageAlone: Percent of county residents with private health coverage alone (no public assistance) (b)\n",
    "PctEmpPrivCoverage: Percent of county residents with employee-provided private health coverage (b)\n",
    "PctPublicCoverage: Percent of county residents with government-provided health coverage (b)\n",
    "PctPubliceCoverageAlone: Percent of county residents with government-provided health coverage alone (b)\n",
    "PctWhite: Percent of county residents who identify as White (b)\n",
    "PctBlack: Percent of county residents who identify as Black (b)\n",
    "PctAsian: Percent of county residents who identify as Asian (b)\n",
    "PctOtherRace: Percent of county residents who identify in a category which is not White, Black, or Asian (b)\n",
    "PctMarriedHouseholds: Percent of married households (b)\n",
    "BirthRate: Number of live births relative to number of women in county (b)\n",
    "(a): years 2010-2016\n",
    "\n",
    "(b): 2013 Census Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TS_iDuCaWBc"
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ads6yBtqaWBc"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rGlRHBtaWBc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEEA8eZRaWBd"
   },
   "source": [
    "### Load the data and display first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mF3Qi2tyaWBd"
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "cancer_data  = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/cancer_death_rate/Training_set_label.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test data\n",
    "test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/cancer_death_rate/Testing_set_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHVlzyz6aWBd"
   },
   "source": [
    "### Perform Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform all EDA and Preprocessing steps on Training Data to keep the Test data unseen and subsequently carry out the same on test data prior predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check info on columns\n",
    "cancer_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except two columns , binnedInc and Geography others are numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNC0AQc6aWBd"
   },
   "outputs": [],
   "source": [
    "# descriptive status \n",
    "cancer_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take the nemeric columns and examine the data distribution\n",
    "col_numeric = list(cancer_data.select_dtypes(exclude='object').columns)\n",
    "len(col_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting histogram of numeric columns\n",
    "i = 1\n",
    "plt.figure(figsize=(20,16))\n",
    "for col in col_numeric:\n",
    "    plt.subplot(8,4,i)\n",
    "    plt.hist(cancer_data[col])\n",
    "    plt.xlabel(col)\n",
    "    i+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some distributions which are skewed and may require transformation to remove the skew. \n",
    "There are some columns like median age male and females which are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check of box plot of above columns to see outliers clearly\n",
    "i = 1\n",
    "plt.figure(figsize=(20,16))\n",
    "for col in col_numeric:\n",
    "    plt.subplot(8,4,i)\n",
    "    sns.boxplot(y=cancer_data[col])\n",
    "    i+=1\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at object data type\n",
    "cat_col = cancer_data.select_dtypes(include='object').columns\n",
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_col:\n",
    "    print(cancer_data[col].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There  are large no of cat for geography ,the count values range for 1 to 2 . So not much info may come out of this column. We can delete the Geography columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Geography column for trg set\n",
    "cancer_data.drop('Geography',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert binnedInc to cat object\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "cancer_data['binnedInc'] = le.fit_transform(cancer_data['binnedInc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets handle null values in data set\n",
    "null_data = cancer_data.isnull().sum()\n",
    "null_data[null_data.values != 0]/len(cancer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that 75 % of data in PctSomeCol18_24 is missing . We can delete this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.drop('PctSomeCol18_24',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['PctEmployed16_Over'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine closely the null values (Percent of county residents ages 16 and over employed (b))\n",
    "cancer_data[cancer_data['PctEmployed16_Over'].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['PctEmployed16_Over'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to have no outlier and can be imputed by mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with mean value\n",
    "cancer_data['PctEmployed16_Over'].fillna(cancer_data['PctEmployed16_Over'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at PctPrivateCoverageAlone column for missing values\n",
    "cancer_data['PctPrivateCoverageAlone'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['PctPrivateCoverageAlone'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well no outliers . Hence we shall impute with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputing with mean value\n",
    "cancer_data['PctPrivateCoverageAlone'].fillna(cancer_data['PctPrivateCoverageAlone'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No null values in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BUILDING A BASE LINE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a base line model for refernce with all attributes as X\n",
    "X1 = cancer_data.drop('TARGET_deathRate',axis=1)\n",
    "y1 = cancer_data['TARGET_deathRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into trg and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1_train,X1_valid,y1_train,y1_valid = train_test_split(X1,y1,test_size=0.3,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RandomForestRegressor as base model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf1 = RandomForestRegressor()\n",
    "rf1.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training MSE\n",
    "y1_pred_trg = rf1.predict(X1_train)\n",
    "metrics.mean_squared_error(y1_train,y1_pred_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test MSE\n",
    "from sklearn import metrics\n",
    "y_pred1_valid = rf1.predict(X1_valid)\n",
    "metrics.mean_squared_error(y1_valid,y_pred1_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5UZz7b9I9eJ"
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNuCKaCkaWBd"
   },
   "source": [
    "### Perform Data Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zj3nJaHaWBd"
   },
   "outputs": [],
   "source": [
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the avg death has no meaning without referring to population\n",
    "#so we create a new col pop_to_avgdeath\n",
    "cancer_data['pop_to_avgdeath'] = cancer_data['popEst2015']/cancer_data['avgDeathsPerYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at new col distribution\n",
    "plt.hist(cancer_data['pop_to_avgdeath'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=cancer_data['pop_to_avgdeath'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the columns using log transformation\n",
    "cancer_data['pop_to_avgdeath'] = np.log(cancer_data['pop_to_avgdeath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cancer_data['pop_to_avgdeath'])\n",
    "plt.show()\n",
    "from scipy.stats import skew \n",
    "print(skew(cancer_data['pop_to_avgdeath']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the distribution looks less skewd. The skew value is 0.92. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop orginal columns\n",
    "cancer_data.drop(columns=['popEst2015','avgDeathsPerYear'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle skewness of data\n",
    "cancer_data['medIncome'] = np.log(cancer_data['medIncome'])\n",
    "plt.hist(cancer_data['medIncome'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew \n",
    "skew(cancer_data['medIncome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skewness = 0 : normally distributed.\n",
    "skewness > 0 : more weight in the left tail of the distribution.\n",
    "skewness < 0 : more weight in the right tail of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cancer_data['PctWhite'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data['PctWhite'] = np.log(cancer_data['PctWhite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hyperparameter tuning on baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling data using minmax scaler after splitting \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X2 = cancer_data.drop('TARGET_deathRate',axis=1)\n",
    "y2 = cancer_data[['TARGET_deathRate']]\n",
    "X2_train,X2_valid,y2_train,y2_valid = train_test_split(X2,y2,test_size=0.3,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_train = X2_train.columns\n",
    "col_target = y2_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler()\n",
    "X_scaled.fit(X2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = X_scaled.transform(X2_train)\n",
    "X2_valid = X_scaled.transform(X2_valid)\n",
    "X2_train = pd.DataFrame(X2_train,columns=col_train)\n",
    "X2_valid = pd.DataFrame(X2_valid,columns=col_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale target variable\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train = target_scaler.transform(y2_train)\n",
    "y2_valid = target_scaler.transform(y2_valid)\n",
    "y2_train = pd.DataFrame(y2_train,columns=col_target)\n",
    "y2_valid = pd.DataFrame(y2_valid,columns=col_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred_trg = grid_search.predict(X2_train)\n",
    "metrics.mean_squared_error(y2_train,y2_pred_trg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_pred_valid = grid_search.predict(X2_valid)\n",
    "metrics.mean_squared_error(y2_valid,y2_pred_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting target on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we shall preprocess test data similar to training data prior using model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jju88gE3aWBe"
   },
   "outputs": [],
   "source": [
    "# drop geography \n",
    "test_data.drop('Geography',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert binnedInc to cat object\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "test_data['binnedInc'] = le.fit_transform(test_data['binnedInc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets handle null values in test data set\n",
    "null_data = test_data.isnull().sum()\n",
    "null_data[null_data.values != 0]/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping PctSomeCol18_24\n",
    "test_data.drop('PctSomeCol18_24',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing with mean value \n",
    "test_data['PctEmployed16_Over'].fillna(test_data['PctEmployed16_Over'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PctPrivateCoverageAlone imputing with mean\n",
    "test_data['PctPrivateCoverageAlone'].fillna(test_data['PctPrivateCoverageAlone'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we create a new col pop_to_avgdeath\n",
    "test_data['pop_to_avgdeath'] = test_data['popEst2015']/test_data['avgDeathsPerYear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the columns using log transformation\n",
    "test_data['pop_to_avgdeath'] = np.log(test_data['pop_to_avgdeath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop orginal columns\n",
    "test_data.drop(columns=['popEst2015','avgDeathsPerYear'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medincome col\n",
    "test_data['medIncome'] = np.log(test_data['medIncome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pct white col\n",
    "cancer_data['PctWhite'] = np.log(cancer_data['PctWhite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_test = test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled = X_scaled.transform(test_data)\n",
    "test_data_scaled = pd.DataFrame(test_data_scaled,columns=col_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat2 = grid_search.predict(test_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat2 = y_hat2.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take inverse transform to get predictions\n",
    "y_hat2 = target_scaler.inverse_transform(y_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(y_hat2.flatten())\n",
    "prediction_df.columns = ['prediction']\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('hari_assignment3_rf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "228.78564304461932 is the score obtained for this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USING FEATURE SELECTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also check and drop duplicates from train and test data\n",
    "cancer_data.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = cancer_data[cancer_data.duplicated()]\n",
    "duplicate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_test = test_data[test_data.duplicated()]\n",
    "duplicate_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicate in both train and test data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy as bp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X = cancer_data.drop('TARGET_deathRate',axis=1)\n",
    "y = cancer_data['TARGET_deathRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_valid = X_valid.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using boruta for feature selection\n",
    "rf_model =  RandomForestRegressor(n_jobs=-1, max_depth=5)\n",
    "feat_selector = bp(rf_model, n_estimators='auto', verbose=0, random_state=42, max_iter = 100, perc = 70)\n",
    "feat_selector.fit(np.array(X_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise it better in the form of a table\n",
    "selected_rfe_features = pd.DataFrame({'Feature':list(X_train.columns),\n",
    "                                      'Ranking':feat_selector.ranking_})\n",
    "selected_rfe_features.sort_values(by='Ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rfe_features.drop([5,4],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns selected by boruta\n",
    "final_col = list(selected_rfe_features['Feature'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform selected train and valid set\n",
    "X_important_train = feat_selector.transform(np.array(X_train))\n",
    "X_important_valid = feat_selector.transform(np.array(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_important_train = pd.DataFrame(X_important_train,columns=final_col)\n",
    "X_important_valid = pd.DataFrame(X_important_valid,columns=final_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform test data\n",
    "test_data_imp = feat_selector.transform(np.array(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_imp = pd.DataFrame(test_data_imp,columns=final_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_important_train.shape,X_important_valid.shape,test_data_imp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do gridserch cv for hyperparameter tuning \n",
    "gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,20),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0,n_jobs=-1)\n",
    "    \n",
    "grid_result = gsc.fit(X_important_train, y_train)\n",
    "best_params = grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.fit(X_important_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_trg = gsc.predict(X_important_train)\n",
    "metrics.mean_squared_error(y_train,y_pred_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_valid = grid_result.predict(X_important_valid)\n",
    "metrics.mean_squared_error(y_valid,y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on test data\n",
    "y_hat = grid_result.predict(test_data_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(y_hat.flatten())\n",
    "prediction_df.columns = ['prediction']\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('hari_assignment3_rf_fs.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "225.02555083369964 is the score(MSE) obtained for this model . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING ABOVE RANDOM FOREST WITH HYPERPARAMETER TUNING AND SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling MinMaxScaler\n",
    "X_scaled = MinMaxScaler()\n",
    "#fit X_important_train with scaler\n",
    "X_scaled.fit(X_important_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_scaled.transform(X_important_train)\n",
    "X_valid = X_scaled.transform(X_important_valid)\n",
    "X_train = pd.DataFrame(X_train,columns=final_col)\n",
    "X_valid = pd.DataFrame(X_valid,columns=final_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = pd.DataFrame(y_train, columns=col_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale target variable\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = pd.DataFrame(y_valid, columns=col_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target_scaler.transform(y_train)\n",
    "y_valid = target_scaler.transform(y_valid)\n",
    "y_train = pd.DataFrame(y_train,columns=col_target)\n",
    "y_valid = pd.DataFrame(y_valid,columns=col_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling test data\n",
    "test_data_scaled_imp = X_scaled.transform(test_data_imp)\n",
    "test_data_scaled_imp = pd.DataFrame(test_data_scaled_imp,columns=final_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do gridserch cv for hyperparameter tuning \n",
    "gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,20),\n",
    "            'n_estimators': (10, 50, 100, 1000),\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0,n_jobs=-1)\n",
    "    \n",
    "grid_result = gsc.fit(X_train, y_train)\n",
    "best_params = grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traing MSE\n",
    "y_pred_trg2 = grid_result.predict(X_train)\n",
    "metrics.mean_squared_error(y_train,y_pred_trg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation MSE\n",
    "y_pred_valid2 = grid_result.predict(X_valid)\n",
    "metrics.mean_squared_error(y_valid,y_pred_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on test data\n",
    "y_hat_2 = grid_result.predict(test_data_scaled_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_2 = target_scaler.inverse_transform(y_hat_2.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(y_hat_2.flatten())\n",
    "prediction_df.columns = ['prediction']\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('hari_assignment3_rf_fs_sc.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "221.57504664347528 is the score obtained for this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM1_hZ8PaWBe"
   },
   "source": [
    "### Try out other Machine Learning Models and Evaluate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall try out xgboost algorithm on the above data set for one more iteration. we will use important features obtained from Boruta and unscaled features and target variables. I am getting error using xgboost with scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WX8K9gDraWBe"
   },
   "outputs": [],
   "source": [
    "import xgboost as xg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use unscaled y_train\n",
    "y_train = target_scaler.inverse_transform(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting and predicting with xgboost regressor\n",
    "xgdmat=xg.DMatrix(X_important_train,y_train)\n",
    "our_params={'eta':0.1,'seed':0,'subsample':0.8,'colsample_bytree':0.8,'objective':'reg:linear','max_depth':3,'min_child_weight':1}\n",
    "final_gb=xg.train(our_params,xgdmat)\n",
    "tesdmat=xg.DMatrix(test_data_imp)\n",
    "pred =final_gb.predict(tesdmat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_xgb = pd.DataFrame(pred.flatten())\n",
    "prediction_df_xgb.columns = ['prediction']\n",
    "prediction_df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_xgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv file\n",
    "prediction_df_xgb.to_csv('hari_assignment3_xgb.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4409.702415442606 is the mean squared error score obtained. So we disregard this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model post multiple iterations was found to be RandomForestRegressor with hyperparameter tuning using GridsearchCV with 28 features obtained from feature selection algorithm Boruta.\n",
    "\n",
    "It is opined that further reduction in MSE can be attempted post transformation of some more features which have some skew. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3: Notebook Submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
